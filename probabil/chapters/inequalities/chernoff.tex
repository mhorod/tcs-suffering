\subsection{Definicja}
Łączymy ze sobą dwie rzeczy -- funkcje tworzące momentów, oraz nierówność Markowa.
\begin{theorem}
    \[
        \forall_{t > 0} : P(X \geq a) = P(e^{tX} \geq e^{ta}) \leq \frac{\expected{e^{tX}}}{e^{ta}}
    \]
    oraz
    \[
        \forall_{t < 0} : P(X \leq a) = P(e^{tX} \geq e^{ta}) \leq \frac{\expected{e^{tX}}}{e^{ta}}
    \]
\end{theorem}
\begin{proof}
    Niezależnie od tego jakie wartości przyjmuje \( X \) oraz ile wynosi \( t \) to \( e^{tX} \) oraz \( e^{ta} \) zawsze będą dodatnie.
    Monotoniczność \( e^{tx} \) przy ustalonym \( t \) zależy jedynie od znaku zatem przejścia między prawdopodobieństwami zachodzą.
    
    Ograniczenie górne uzyskujemy korzystając z nierówności Markowa zastosowanej do (dodatnich) wartości \( e^{tX} \) oraz \( e^{ta} \).
\end{proof}

\subsection{Rzuty monetą}

\begin{definition}
    \textbf{Próbami Poissona} nazywany ciąg zmiennych losowych \( X_1, \dots, X_n \), dla których
    \[
        P(X_i = 1) = p_i \land P(X_i = 0) = 1 - p_i
    \]
    Ponadto definiujemy 
    \[
        \mu = \expected{\sum X_i} = \sum \expected{X_i} = \sum p_i
    \]
\end{definition}

\begin{lemma}
    \label{independent-poisson-trials-abs-bound}
    Niech \( X_1, \dots X_n \) będą niezależnymi próbami Poissona oraz \( X = \sum X_i, \mu = \expected{X} \). 
    
    Wtedy dla \( 0 < \delta < 1 \)
    \[
        P(\abs{X - \mu} \geq \delta \mu) \leq 2 \exp\pars{\frac{-\mu\delta^2}{3}}
    \]
\end{lemma}
\begin{proof}
    \[
        P(\abs{X - \mu} \geq \delta\mu) \leq P\pars{X \geq (1 + \delta)\mu} + P\pars{X \leq (1 - \delta)\mu}
    \]
\end{proof}

\subsection{Przypadki specjalne}

\begin{theorem}
    Niech \(X_1,\ldots,X_n\) będą niezależnymi zmiennymi takimi, że \(P \left( X_i=1 \right) = p_i\) oraz \(P \left( X_i = 0 \right) = 1-p_i\). Niech \(X = \sum_{i=1}^{n} X_i\) oraz \(\mu = \mathbb{E}\left[ X \right] \). Wtedy:
    \begin{enumerate}
        \item \(\forall_{\delta > 0} \ P \left( X \ge \left( 1+\delta \right) \mu \right) \le \left( \frac{e^{\delta}}{ \left( 1+\delta \right) ^{1+\delta}} \right) ^{\mu} \) 
        \item \(\forall_{\delta \in (0,1]} \ P \left( X \ge \left( 1+\delta \right) \mu \right) \le e^{-\frac{\mu\delta^2}{3}} \)            
        \item \(\forall_{R \ge 6\mu} \ P \left( X \ge R \right) \le 2^{-R} \).
    \end{enumerate}
\end{theorem}
\begin{proof}
    Liczymy funkcję tworzącą \[M_{X_i} \left( t \right) = \mathbb{E}\left[ e ^{tX_i} \right] = p_ie^{t}+ \left( 1-p_i \right) = 1 + p_i \left( e^{t}-1 \right) \le e^{p_i \left( e^{t}-1 \right) } .\]

    Zatem \[M_X \left( t \right) = \prod_{i=1}^{n} M_{X_i} \left( t \right) \le \prod_{i=1}^{n} e^{p_i \left( e^{t}-1 \right) } = e^{ \left( e^{t}-1 \right) \mu}  .\]

    Ustalmy \(t > 0\), mamy \[P \left( X \ge \left( 1+\delta \right) \mu \right) = P \left( e ^{tX}\ge e^{t \left( 1+\delta \right) \mu} \right) \le \frac{\mathbb{E}\left[ e^{tX} \right] }{e^{t \left( 1+\delta \right) \mu}} \le  \frac{e^{\left(e^{t}-1\right)\mu}}{e^{t \left( 1+\delta \right) \mu}} .\] 
    Niech \(t = \ln \left( 1+\delta \right) > 0\). Wychodzi nam \(P \left( X \ge \left( 1+\delta \right) \mu \right) \le  \left(\frac{e^{1+\delta-1}}{ \left( 1+\delta \right) ^{1+\delta}}\right) ^{\mu}\), co kończy dowód pierwszej części.

    Punkt drugi dowodzimy korzystając z pierwszego, wystarczy pokazać, że dla \(\delta \in (0,1]\) jest \[\frac{e^{\delta}}{ \left( 1+\delta \right) ^{1+\delta}} \le e^{- \frac{\delta^2}{3}}.\]

    Logarytmujemy stronami, chcemy pokazać, że \(\delta - \left( 1+ \delta \right)\ln \left( 1+\delta   \right) + \frac{\delta^2}{3} \le 0\). Oznaczmy lewą stronę przez \(f \left( \delta \right) \). Liczymy pochodne:
    \[f' \left( \delta \right) = 1- 1\cdot \ln \left( 1+\delta \right) - \frac{1+\delta}{1+\delta} + \frac{2}{3}\delta = - \ln \left( 1+\delta \right) + \frac{2}{3}\delta,\]
    \[f'' \left( \delta \right) = -\frac{1}{1+\delta} + \frac{2}{3}.\]

    \(f' \left( 0 \right) = 0\), a potem maleje do \(\delta = \frac{1}{2}\) (tam druga pochodna się zeruje, przedtem ujemna), potem rośnie, ale \(f' \left( 1 \right) < 0\), więc jest ujemna na całym \((0,1]\).

    \(f \left( \delta \right) \) tylko maleje na \((0,1]\), więc nierówność działa, bo \(f \left( 0 \right) = 0\).
        
    Dowodząc punkt trzeci zakładamy \(R \ge 6\mu\). Niech \(R = \left( 1+\delta \right) \mu\), czyli \(\delta = \frac{R}{\mu}-1 \ge 5\).

    \[P \left( X \ge \left( 1+\delta \right) \mu \right) \le \left( \frac{e^{\delta}}{ \left( 1+\delta \right) ^{1+\delta}} \right) ^{\mu} \le \left( \frac{e}{1+\delta} \right) ^{ \left( 1+\delta \right) \mu} \le \left( \frac{1}{2} \right) ^{R} = 2 ^{-R}.\]
\end{proof}

\begin{theorem}
    Niech \(X_1,\ldots,X_n\) będą niezależnymi zmiennymi takimi, że \(P \left( X_i = 1 \right) = p_i\) oraz \(P \left( X_i = 0 \right) = 1-p_i\). Niech \(X = \sum_{i=1}^{n} X_i\), \(\mu = \mathbb{E}\left[ X \right] \) Wtedy dla każdego \(\delta \in (0,1)\) zachodzi:
    \begin{enumerate}
        \item \( P \left( X \le \left( 1-\delta \right) \mu \right) \le \left( \frac{e^{-\delta}}{ \left( 1-\delta \right) ^{1-\delta}} \right) ^{\mu}\) 
        \item \(P \left( X \le \left( 1-\delta \right) \mu \right) \le e^{-\frac{\mu\delta^2}{2}}\).
    \end{enumerate}
\end{theorem}
\begin{proof}
    Dowód identyczny jak w poprzednim twierdzeniu, wybieramy \(t = \ln \left( 1-\delta \right) < 0\) i korzystamy z tego, że \(e^{-z}\) jest antymonotoniczne. Drugiego punktu ponownie dowodzimy licząc pochodne i na ich podstawie dowodząc odpowiedniej nierówności.
\end{proof}

\begin{theorem}
    Niech \(X_1,\ldots,X_n\) będą niezależnymi zmiennymi losowymi o rozkładzie prawdopodobieństwa \(P \left( X_i = 1 \right) = P \left( X_i = -1 \right) = \frac{1}{2}\). Niech \(X = \sum_{i=1}^{n} X_i\). Dla każdego \(a>0\) mamy \(P \left( X \ge a \right) \le e^{-\frac{a^2}{2n}}\). Zauważmy, że nie ma sensu rozważać tu odchyleń multiplikatywnych, bo \(\mathbb{E}\left[ X \right] = 0\).
\end{theorem}
\begin{proof}
    Mamy \(\mathbb{E}\left[ e^{tX_{i}} \right] = \frac{1}{2}e^{-t} + \frac{1}{2}e^{t}\).

    Rozwijamy w szereg Taylora:
    \[e^{t} = 1 + t + \frac{t^2}{2} + \ldots + \frac{t ^{i}}{i!} + \ldots\]
    \[e^{-t} = 1 -t + \frac{t^2}{2} + \ldots + \left( -1 \right) ^{i} \frac{t ^{i}}{i!} + \ldots\]

    Z tego wynika \[\mathbb{E}\left[ e^{tX_i} \right] = \sum_{i\ge 0}^{} \frac{t ^{2i}}{ \left( 2i \right) !} \le \sum_{i\ge 0}^{} \frac{ \left( \frac{t^2}{2} \right) ^{i}}{i!} = e^{\frac{t^2}{2}},\] gdzie w nierówności wyciągnęliśmy \(2\) z dwukrotności każdej liczby od \(1\) do \(i\), a pozostałe składniki zignorowaliśmy.

    Zatem \(\mathbb{E}\left[ e^{tX} \right] = \prod_{i=1}^{n} \mathbb{E}\left[e^{tX_i}\right] \le e^{\frac{t^2n}{2}} \).

    Dostajemy \(P \left( X\ge a \right) = P \left( e^{tX}\ge e^{ta} \right) \le \frac{\mathbb{E}\left[ e^{tX} \right] }{e^{ta}} \le e^{t^2n\cdot \frac{1}{2}-ta} = e^{a^2 \frac{1}{n} \frac{1}{2} - \frac{a^2}{n}} = e^{-a^2\cdot \frac{1}{2n}}\), gdzie podstawiliśmy \(t = \frac{a}{n} > 0\).
\end{proof}

\begin{theorem}
    Niech \(Y_1,\ldots,Y_n\) będą niezależnymi indykatorami \(P \left( Y_i = 0 \right) = P \left( Y_i = 1 \right) = \frac{1}{2}\). Niech \(Y = \sum_{i=1}^{n} Y_i\), \(\mu = \mathbb{E}\left[ Y \right] = \frac{n}{2}\). Wtedy 
    \begin{enumerate}
        \item \(\forall_{a>0} \ P \left( Y \ge \mu+a \right) \le e^{-\frac{2a^2}{n}} \)
        \item \(\forall_{\delta > 0} \   P \left( Y \ge \left( 1+\delta \right) \mu \right) \le e ^{-\delta^2\mu}\)
    \end{enumerate}
\end{theorem}
\begin{proof}
    Bierzemy \(Y_i = \frac{X_i + 1}{2}\). Mamy \(P \left( X_i = 1 \right) = P \left( X_i = -1 \right) = \frac{1}{2}\). Dla  \(X = \sum_{i=1}^{n} X_i = 2Y-2\mu\) mamy \[ P \left( Y \ge \mu+a \right) = P \left( X \ge 2a \right) \le e^{-\frac{2a^2}{n}}  \] 
    oraz \[ P \left( Y \ge \left( 1+\delta \right) \mu \right) = P \left( X \ge 2\delta\mu \right) \le e^{- \frac{2\delta^2\mu^2}{n}} = e^{-\delta^2\mu}. \] 
\end{proof}


\subsection{Estymacja parametru}

\begin{definition}
    Mówimy, że \([\hat{p}-\delta , \hat{p}+\delta]\) jest \( \left( 1-\gamma \right) \) przedziałem ufności dla parametru \(p\), jeśli \(P \left( p \in [\hat{p}-\delta, \hat{p}+\delta] \right) \ge 1-\gamma\). Chcemy, żeby \(n,\gamma,\delta\) były małe, ale musi być między nimi jakiś balans.
\end{definition}

Bierzemy z dużej populacji próbkę wielkości \(n\) wybraną w sposób jednostajny. \(p\) to nieznana wartość -- szukane prawdopodobieństwo, które chcemy szacować (np. prawdopodobieństwo jakiejś mutacji genetycznej). Niech zmienna losowa \(X = \hat{p}n\) oznacza liczbę wystąpień tej mutacji w naszej próbce. Spodziewamy się, że jak \(n\) rośnie, to \(\hat{p} \to p\).

Jeśli \(p < \hat{p}-\delta\), to \(X = n\hat{p} > n \left( p+\delta \right) = np \cdot \left( 1+\frac{\delta}{p} \right) \) .

Jeśli \(p > \hat{p}+\delta\), to \(X = n \hat{p} < n \left( p-\delta \right) = np \cdot \left( 1-\frac{\delta}{p} \right) \).

Mamy \(\mathbb{E}\left[ X \right] = np\), a więc Czernow daje 
\begin{align*}
    P \left( p \notin \left[ \hat{p}-\delta, \hat{p}+\delta \right]  \right) &= P \left( X < np \left( 1-\frac{\delta}{p} \right)  \right) + P \left( X > np \left( 1+\frac{\delta}{p} \right)  \right) \\
                                                                             &\le 2\cdot e^{-np \cdot \left( \frac{\delta}{p} \right) ^2 \cdot \frac{1}{3}} = 2\cdot e ^{-n \frac{\delta^2}{p}\cdot \frac{1}{3}} \le 2\cdot e^{-n\frac{\delta^2}{3} } = \gamma .
\end{align*}
Pod koniec wzięliśmy \(p=1\), bo daje najgorsze ograniczenie. W ten sposób związaliśmy ze sobą wartości \(n,\gamma,\delta\).


\subsection{Problem Set Balancing}

    Mamy macierz \(n\times m\) wypełnioną wartościami z \(\{0,1\} \).

    \[ \begin{bmatrix} a_1 & \ldots & a_{1m} \\ \vdots & \ddots & \vdots \\ a_{n 1} & \ldots & a_{nm} \end{bmatrix}  \] 

    Każdy wiersz to jakaś cecha osoby, kolumna to osoba. Chcemy przeprowadzić jakieś badanie, a do tego potrzebujemy zrobić grupę badawczą i kontrolną, które będą możliwie identyczne (to znaczy o podobnym zagęszczeniu wszystkich cech).

    Mnożymy tę macierz \(A\) przez wektor \(\overline{b} \in \{-1,1\} ^{m}\) (umieszczenie kolejnych osób w jednej lub drugiej grupie) i dostajemy wektor \(\overline{c}\), w którym będą różnice między ilością osób z daną cechą między grupami. Chcemy, żeby norma \(\left\|\overline{c}\right\|_{\infty}\) była jak najmniejsza.

    Wektor \(\overline{b}\) wyznaczamy, losując.

\begin{theorem}
    Dla losowego \(\overline{b}\) (każda współrzędna niezależnie, jednostajnie z \(\{-1,1\} \)) zachodzi 
    \[ P \left( \left\|A \overline{b}\right\|_{\infty}  \ge \sqrt{4m \ln n}  \right) \le \frac{2}{n}. \] 
\end{theorem}
\begin{proof}
    Niech \(i\)-ty wiersz \(\overline{a_i} = a_{i 1}\ldots a_{im}\) ma w sobie \(k\) jedynek. \(Z = \sum_{j=1}^{m} a_{ij}b_j\) jest sumą \(k\) zmiennych losowych, które z równym prawdopodobieństwem przyjmują \(1\) i \(-1\).

    Mamy zatem \(P \left( \left|Z_i\right|\ge  \sqrt{4m \ln n}  \right) \le 2 e^{\frac{-4m \ln n}{2k}} \le \frac{2}{n^2}\), ostatnia nierówność wynika z \(m \ge k\). Jest to ograniczenie dla jednego wiersza, dla wszystkich wierszy dostajemy z union bounda ograniczenie \(\frac{2}{n}\).
\end{proof}

