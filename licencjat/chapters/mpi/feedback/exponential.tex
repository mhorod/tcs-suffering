\begin{definition}
    \textbf{Rozkładem wykładniczym} z parametrem \( \lambda \) nazywamy rozkład zadany dystrybuantą
    \[
        F(x) = \begin{cases}
            1 - e^{-\lambda x} & \text{ dla } x \geq 0 \\
            0 & \text{ dla } x < 0
        \end{cases}
    \]
\end{definition}

Gęstość rozkładu wykładniczego \( f = F' \) wynosi natomiast \( f(x) = \lambda e^{-\lambda x} \).

\begin{lemma}
 Wartość oczekiwana rozkładu wykładniczego o parametrze \( \lambda \) wynosi \( \frac{1}{\lambda} \)
\end{lemma}
\begin{proof}
    \[
        \int_{0}^{\infty} \lambda x e^{-\lambda x} \; dx = \lambda \int_{0}^{\infty} x e^{-\lambda x} \; dx
    \]
    
    Lecimy całkując przez części: 
    
    \begin{itemize}
        \item \(u =  x\)
        \item \(v' = e^{-\lambda x}\) 
    \end{itemize}
    
    więc:
    
    \begin{itemize}
        \item \(u' = 1\)
        \item \(v = \frac{-1}{\lambda} e^{-\lambda x}\)
    \end{itemize}
    
    \[
        \int x e^{-\lambda x} \; dx = \frac{-x}{\lambda} e^{-\lambda x} - \int \frac{-1}{\lambda} e^{-\lambda x}
    \]
    
    z kolei:
    
    \[ 
        \int \frac{-1}{\lambda} e^{-\lambda x} = \frac{-1}{\lambda} \int e^{-\lambda x} = \frac{-1}{\lambda} \pars{\frac{-1}{\lambda} e^{-\lambda x} + c} = \frac{1}{\lambda^2} e^{-\lambda x} - \frac{1}{\lambda}
    \]
    
    a zatem 
    
    \[ 
        \lambda \int_{0}^{\infty} x e^{-\lambda x} \; dx = \lambda \Big\rvert_{0}^{\infty} \pars{\frac{-x}{\lambda} e^{-\lambda x} - \frac{1}{\lambda^2} e^{-\lambda x} + \frac{1}{\lambda} } = \lambda \cdot \frac{1}{\lambda^2} = \frac{1}{\lambda}
    \]
\end{proof}


\begin{theorem}[Lemat 8.4 P\&C]
    Rozkład wykładniczy jest \textbf{bez pamięci} tzn.
    \[ 
        P(X > s + t \mid X > t) = P(X > s)
    \]
\end{theorem}
\begin{proof}
    \begin{align*}
            P(X > s + t \mid X > t) &= \frac{P(X > s + t)}{P(X > t)} \\
            &= \frac{1 - P(X \leq s + t)}{1 - P(X \leq t)} \\
            &= \frac{\exp(-\lambda(s + t))}{\exp(-\lambda t)} \\
            &= e^{-\lambda s} = P(X > s)
    \end{align*}
\end{proof}

Jest to bardzo przydatna własność, bowiem sprawia, że możemy ,,resetować'' zmienną o której wiemy, że ma większą wartość niż ustalona.

\begin{theorem}[Lemat 8.5 P\&C]
\label{lemat8.5}
Jeśli \(X_1, X_2\) są \textbf{niezależnymi} zmiennymi losowymi o rozkładzie geometrycznym o parametrach (odpowiednio) \(\lambda_1\) i \(\lambda_2\) to zmienna losowa \(Y\) będąca ich minimum jest zmienną o rozkładzie geometrycznym o parametrze \( \lambda_1 + \lambda_2 \).
    
Dodatkowo, prawdopodobieństwo że \(X_1 = Y\) (w sensie: że to \(X_1\) będzie mniejsze) wynosi \( \frac{\lambda_1}{\lambda_1 + \lambda_2} \). Analogicznie, dla \(X_2\) prawdopodobieństwo to wyniesie \( \frac{\lambda_2}{\lambda_1 + \lambda_2} \).
\end{theorem}

\begin{proof}
    Policzmy sobie dystrybuantę zmiennej losowej \(Y\) (w nieco śmieszny sposób): 
    
    \[ 
        1 - F_Y(x) = \mathrm{P}(x \leq Y) = \mathrm{P}(x \leq X_1 \land x \leq X_2) = \mathrm{P}(x \leq X_1) \cdot \mathrm{P}(x \leq X_2) 
    \]
    \[ 
        P(x \leq X_1) \cdot P(x \leq X_2) = (1 - F_{X_1}(x)) \cdot (1 - F_{X_2}(x))
    \]
    Jako, że dla zmiennych \(X\) rozkładu wykładniczego \(F_{X}(x) = 1 - e^{- \lambda x}\) to \(1 - F_{X}(x) = e^{-\lambda x}\). Zatem:
    \[
    (1 - F_{X_1}(x)) \cdot (1 - F_{X_2}(x)) = e^{-\lambda_1 x} \cdot e^{-\lambda_2 x} = e^{-(\lambda_1 + \lambda_2)x} = 1 - F_Y(x)
    \]
    czyli 
    \[ 
        F_Y(x) = 1 - e^{-(\lambda_1 + \lambda_2)x}
    \]
    
    skąd \(Y\) jest zmienną rozkładu wykładniczego z parametrem \( \lambda_1 + \lambda_2\). Teraz pozostaje pokazać, że:
    
    \[
        P(X_1 \leq X_2) = \frac{\lambda_1}{\lambda_1 + \lambda_2}
    \]
    
    Zatem liczymy:
    
    \begin{align*} 
        \mathrm{P}(X_1 \leq X_2) = \int_{x_2 = -\infty}^{\infty} \int_{x_1 = -\infty}^{x_2} f_{X_{1}X_{2}}(x_1, x_2) \; dx_{1}dx_{2} &= \\ 
        \int_{x_2 = -\infty}^{\infty} f_{X_{2}}(x_2) \int_{x_1 = -\infty}^{x_2} f_{X_1}(x_1) \; dx_{1}dx_{2} &=  \\
        \int_{x_2 = 0}^{\infty} \lambda_2 e^{-\lambda_{2}x_2} \int_{x_1 = 0}^{x_2} \lambda_1 e^{-\lambda_1 x_1} \; dx_{1}dx_{2} &= \\
        \lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \int_{x_1 = 0}^{x_2} e^{-\lambda_1 x_1} \; dx_{1}dx_{2} &= \\ 
        \lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \pars{\Big\rvert_{0}^{x_2} \frac{-1}{\lambda_1} e^{-\lambda_1 x_1} } \;  dx_{2} &= \\ 
        \lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_{2}x_2} \pars{\frac{-1}{\lambda_1} e^{-\lambda_1 x_2} - \frac{-1}{\lambda_1} e^0} \; dx_2 &= \\ 
        \lambda_1 \lambda_2 \int_{x_2 = 0}^{\infty} e^{-\lambda_2 x_2} \frac{-1}{\lambda_1} \pars{e^{-\lambda_1 x_2} - 1} \; dx_2 &= \\ 
        -\lambda_2 \int_{x_2=0}^{\infty} e^{-\lambda_2 x_2} \pars{e^{-\lambda_1 x_2} - 1} \; dx_2 &= \\ 
        -\lambda_2 \int_{x_2=0}^{\infty} e^{-\lambda_2x_2 - \lambda_1 x_2} - e^{-\lambda_2 x_2} \; dx_2 &= \\ 
         -\lambda_2 \int_{x_2=0}^{\infty} e^{-x_2\pars{\lambda_2 + \lambda_1}} - e^{-\lambda_2 x_2} \; dx_2 &= \\ 
        -\lambda_2 \pars{\int_{x_2=0}^{\infty} e^{-x_2\pars{\lambda_2 + \lambda_1}} \; dx_2 - \int_{x_2=0}^{\infty} e^{-\lambda_2 x_2} \; dx_2} &= \\
        -\lambda_2 \pars{\pars{\Big\rvert_{x_2=0}^{\infty} \frac{-1}{\lambda_1 + \lambda_2} e^{-x_2(\lambda_1 + \lambda_2)}} - \pars{\Big\rvert_{x_2=0}^{\infty} \frac{-1}{\lambda_2} e^{-\lambda_2 x_2}}} &= \\ 
        -\lambda_2 \pars{\frac{1}{\lambda_1 + \lambda_2} - \frac{1}{\lambda_2}} &= \\ 
        -\lambda_2 \pars{\frac{\lambda_2}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}} - \frac{\lambda_1 + \lambda_2}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}}} &= \\
        (-\lambda_2)\cdot \frac{-\lambda_1}{\lambda_2 \cdot \pars{\lambda_1 + \lambda_2}} &= \\ 
        \frac{\lambda_1}{\lambda_1 + \lambda_2}
    \end{align*}
\end{proof}