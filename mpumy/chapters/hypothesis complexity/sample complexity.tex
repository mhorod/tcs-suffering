\section{Złożoność próbkowa}

\begin{definition}
	Mówimy, że hipoteza \( h_S \) utworzona na próbce \( S \) jest \textbf{spójna} jeśli
	błąd empiryczny jest zerowy czyli \( \forall_{x \in S} : h_S(x) = c(x) \).
\end{definition}

\begin{theorem}
	Niech \( H \subseteq Y^X \) będzie skończonym zbiorem hipotez a \( c \in H \) będzie pojęciem. Jeśli dla dowolnej próbki \( S \) zwraamy spójną hipotezę \( h_S \) to dla dowolnych \( \varepsilon, \delta > 0 \) o ile
	\[
		m \geq \frac{1}{\varepsilon}\pars{\ln \card{H} + \ln \frac{1}{\delta}}
	\]
	to
	\[
		p(R(h_S) \leq \varepsilon) \geq 1 - \delta
	\]
\end{theorem}
\begin{proof}
	Pokażemy że możemy dobrać takie \( m \) dla którego
	\[
		p(R(h_S) > \varepsilon) \leq \delta
	\]

	Skoro \( R(h_S) > \varepsilon \) to z definicji ryzyka \( p(h_S(x) \neq c(x)) > \varepsilon \).

	Ale skoro \( h_S \) jest spójna to prawdopodobieństwo wylosowania spójnego z nią \( S \) wynosi co najwyżej \( (1 - \varepsilon)^m \).

	Pesymistycznie mamy \( \card{H} - 1 \) hipotez z których każda może być spójna, ale niepoprawna -- algorytm może wybrać dowolną z nich.

	Prawdopodobieństwo, że \( S \) jest spójny z dowolną z nich ograniczamy od góry poprzez union bound:
	\[
		p(R(h_S) > \varepsilon) \leq (\card{H} - 1) (1 - \varepsilon)^m \leq \card{H}\exp(\varepsilon m)
	\]

	Przekształcając dostajemy tezę:
	\begin{align*}
		\card{H}\exp(\varepsilon m) & \leq \delta                                                          \\
		\exp(\varepsilon m)         & \leq \frac{\delta}{\card{H}}                                         \\
		\varepsilon m               & \leq \ln \delta - \ln \card{H}                                       \\
		m                           & \leq \frac{1}{\varepsilon}\pars{\ln \delta - \ln \card{H}}           \\
		m                           & \geq \frac{1}{\varepsilon}\pars{\ln \card{H} - \ln \delta}           \\
		m                           & \geq \frac{1}{\varepsilon}\pars{\ln \card{H} + \ln \frac{1}{\delta}}
	\end{align*}
\end{proof}