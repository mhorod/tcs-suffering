\section{Metoda najmniejszych kwadratów}

Bierzemy model bez funkcji bazowych tj.
\[
    h_\theta(x) = \theta_0 + \sum_{i=1}^k \theta_i x_i
\]

oraz kwadratową funkcję straty

\[
    J(\theta) = \frac{1}{2} \sum_{i=1}^m \pars{h_\theta(x^{(i)}) - y^{(i)}}^2
\]

Tworzymy \textbf{macierz planowania} \( X \)
\[
    X = \begin{bmatrix}
        1 & x^{(1)}_1 & \hdots & x^{(1)}_k \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x^{(m)}_1 & \hdots & x^{(m)}_k \\
    \end{bmatrix}
\]

oraz \textbf{wektor zmiennej objaśnianej} \( y \)
\[
    y = \begin{bmatrix}
        y^{(1)} \\
        \vdots \\
        y^{(m)}
    \end{bmatrix}
\]

Możemy teraz elegancko zapisać
\[
    J(\theta) = \frac{1}{2}\pars{X\theta - y}^T\pars{X\theta - y}
\]

Minimalizację robimy albo stosując spadek wzdłuż gradientu albo obliczając analitycznie
\[
    \theta = (X^TX)^{-1}X^Ty
\]