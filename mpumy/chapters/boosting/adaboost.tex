\section{AdaBoost}

Zaczynamy od przydzielenia wszystkim obserwacjom takiej samej wagi \( w^{(i)}_1 = \frac{1}{m} \)

W \(t\)-tej iteracji wybieramy ten klasyfikator, który popełnia najmniejszy ważony błąd \( \epsilon_t \leq \frac{1}{2} \)

Oznaczamy ten klasyfikator przez \( h_t \) i nadajemy mu wagę
\[
    \alpha_t = \frac{1}{2} \ln \frac{1 - \epsilon_t}{\epsilon_t} \in [0, \infty)
\]

Następnie modyfikujemy wagi obserwacjom:
\[
    w^{(i)}_{t + 1} = \begin{cases}
        w^{(i)}_t \cdot \exp(\alpha_t) & \text{ gdy } h_t(x^{(i)}) \neq y^{(i)} \\
        w^{(i)}_t \cdot \exp(-\alpha_t) & \text{ gdy } h_t(x^{(i)}) = y^{(i)} \\
    \end{cases}
\]

W ten sposób wagi dla dobrze trafionych obserwacji maleją, a nietrafionych rosną.

Po każdej iteracji normalizujemy wagi tak aby ich suma wynosiła 1.

Zatrzymujemy jeśli zajdzie jeden z przypadków:
\begin{itemize}
    \item Wykonamy \( T \) iteracji
    \item Znajdziemy \( h_t \) z zerowym błędem
    \item Wszystkie klasyfikatory mają ważony błąd \( \frac{1}{2} \) -- wtedy \( \alpha_t = 0 \) i już nic się nie zmieni.
\end{itemize}

Zachodzi twierdzenie:
\begin{theorem}
Po \( T \) iteracjach ryzyko empiryczne klasyfikatora zwróconego przez AdaBoost jest ograniczony
\[
    \widehat R(h) \leq \exp\pars{-2\sum_{t=1}^T \pars{\frac{1}{2} - \epsilon_t}^2}
\]
ponadto jeśli \( \forall_{t \in \set{1, \dots, T}} : \gamma \leq \frac{1}{2} - \epsilon_t \)
to
\[
    \widehat R(h) \leq \exp\pars{-2\gamma^2T}
\]
\end{theorem}

\subsection{Wady i zalety}

Zalety
\begin{itemize}
    \item prosty
    \item tylko jeden hiperparametr -- \( T \)
    \item nie przeucza się
    \item działa dla różnych słabych klasyfikatorów
\end{itemize}

Wady:
\begin{itemize}
    \item podatny na obserwacje odstające
    \item czasem nie działa jeśli źle dobierzemy klasyfikatory
\end{itemize}