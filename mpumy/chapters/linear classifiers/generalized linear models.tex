\section{Uogólnione modele liniowe}

\subsection{Wykładnicza rodzina rozkładów}

\begin{definition}
	Niech \( P_\Theta = \set{p(y \given \theta) : \theta \in \Theta} \) będzie parametryczną rodziną rozkładów.

	Mówimy, że \( P_\Theta \) jest \textbf{wykładniczą rodziną rozkładów} jeśli dla każdego \( \theta \in \Theta \) istnieje wektor \( \eta \) (parametr naturalny) oraz funkcje \( a(\eta), b(y), T(y) \) takie, że:
	\[
		p(y \given \eta) = b(y)\exp\pars{\eta^T T(y) - a(\eta)}
	\]
	(nie mylić z rodziną wykładniczych rozkładów, to nie o to chodzi).
\end{definition}

\subsubsection{Rozkład Bernoulliego}

Mamy rozkład z jednym parametrem \( \phi \), \( p(y \given \phi) = \phi^y(1-\phi)^{1-y} \)

Rozpisujemy
\begin{align*}
	\phi^y(1-\phi)^{1-y}
	 & = \exp\pars{y \ln \phi + (1 - y) \ln (1 - \phi)}                \\
	 & = \exp\pars{y (\ln \phi - \ln (1 - \phi)) + \ln (1 - \phi)}     \\
	 & = \exp\pars{y \ln \frac{\phi}{\ln (1 - \phi)} + \ln (1 - \phi)} \\
\end{align*}
W efekcie czego dostajemy:
\begin{itemize}
	\item \( \eta = \ln \frac{\phi}{1 - \phi} \)
	\item \( T(y) = y \)
	\item \( a(\eta) = -\ln(1 - \phi) = \ln(1 + e^\eta) \)
	\item \( b(y) = 1 \)
\end{itemize}

\subsubsection{Rozkład normalny}


\subsection{Uogólnione modele liniowe}

Chcemy przewidywać wartości \( y \) na podstawie obserwacji \( x \) z jakiejś przestrzeni \( X \times Y \)

Założenia modelu:
\begin{itemize}
	\item Dla wejścia \( x \) oraz parametru \( \theta \) zmienna \( y \given x, \theta \) pochodzi z wykładniczej rodziny rozkładów o parametrze \( \eta \)
	\item \( \eta = \theta^T x \)
	\item \( h_\theta(x) = \expected{T(y) \given x, \theta} \)
	\item \( g(\eta) = \expected{T(y) \given \eta} \) to \textbf{kanoniczna funkcja odpowiedzi}
	\item \( g^{-1}(\eta) \) to \textbf{kanoniczna funkcja łącząca}
\end{itemize}

\subsubsection{Regresja liniowa}

W regresji liniowej z szumem gaussowskim mamy \( p(y \given x) \sim \Normal(\mu, \sigma^2) \)

Niech \( \sigma^2 \) będzie ustalone.
Wtedy \( \set{\Normal(\mu, \sigma^2) : \mu \in \real } \) jest rodziną wykładniczą.

Kładziemy \( \eta = \mu, T(y) = y, \expected{T(y) \given x, \theta} = \mu \), \( h_\theta(x) = \mu = \eta = \theta^T x \)

\subsubsection{Regresja logistyczna}

W regresji logistycznej mamy z kolei \( p(y \given x) \sim \Bern(\phi) \), co jest elegancką rodziną wykładniczą.

Mamy \( \eta = \ln \frac{\phi}{1 - \phi} \) zatem \( \phi = \frac{1}{1 + \exp(-\eta)} \)

Ponadto, ponieważ jest to indykator to mamy \( \expected{y \given x, \theta} = \phi \)
czyli mamy
\[
	h_\theta(x) = \phi = \frac{1}{1 + \exp(-\theta^T x)}
\]


\subsubsection{Regresja wieloklasowa (softmax)}
Załóżmy, że chcemy klasyfikować do więcej niż jednej klasy tj, \( Y = \set{1, \dots, K} \).

Mamy więc
\[
	p(y = i \given x) = \phi_i = \prod_{j=1}^K \phi_j^{\indicator{y=j}}
\]
przy czym
\[
	\sum_{i=1}^K \phi_i = 1
\]
aby dostać wykładniczą rodzinę przekształcamy:
\begin{align*}
	P(y \given x)
	 & = \exp\pars{\sum_{j=1}^K \indicator{y = j} \ln \phi_j}                                 \\
	 & = \exp\pars{\ln \phi_K + \sum_{j=1}^K \indicator{y = j} (\ln \phi_j - \ln \phi_K)}     \\
	 & = \exp\pars{\ln \phi_K + \sum_{j=1}^{K-1} \indicator{y = j} \ln \frac{\phi_j}{\phi_K}} \\
\end{align*}
Sprowadzamy do takiej a nie innej postaci, bo możemy, i ma ona fajne konsekwencje.

Dostajemy
\begin{itemize}
	\item \( \eta_i  = \ln \frac{\phi_i}{\phi_K} \)
	\item \( a(\eta) = -\ln \phi_K \)
	\item \( b(y) = 1 \)
	\item \( T(i) \) -- wektor długości \( K - 1 \) z jedynką na \(i\)-tym miejscu, \( T(K) = 0 \)
\end{itemize}

Zauważamy, że
\[
	\frac{\phi_i}{\phi_K} = \exp(\eta_i)
\]
a ponadto
\[
	\frac{1}{\phi_K} = \sum_{i=1}^K \frac{\phi_i}{\phi_K} = \sum_{i=1}^K \exp(\eta_i)
\]
z czego
\[
	\phi_i = \frac{\exp(\eta_i)}{\sum_{i=1}^K \exp(\eta_i)}
\]

W takim razie
\begin{align*}
	h_\theta(x)
	 & = \expected{T(y) \given x, \theta} \\
	 & = [\phi_1, \dots, \phi_{K-1}]^T
\end{align*}
Czyli
\begin{align*}
	\theta_i
	 & = \phi_i                                                     \\
	 & = \frac{\exp(\eta_i)}{\sum_{i=1}^K \exp(\eta_i)}             \\
	 & = \frac{\exp(\theta^T_i x)}{\sum_{i=1}^K \exp(\theta^T_i x)}
\end{align*}