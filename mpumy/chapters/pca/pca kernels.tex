\section{Jądrowa wersja PCA}

Liniowe odwzorowania mogą być czasem niewystarczające, będziemy się zatem starali otrzymać nieliniowe główne składowe.

\subsection{Zależność między macierzą Grama a macierzą kowariancji}

\begin{lemma}
	Niech \( x^{(1)}, \dots x^{(m)} \in \real^d \) będą wyśrodkowanymi danymi.
	Niech \( K = XX^T \) będzie macierzą Grama dla tych danych
	oraz \( \lambda \in  \real \) oraz \( a \in \real^m \) będą takie, że \( Ka = \lambda a \)

	Niech
	\[
		v = X^Ta
	\]

	Wtedy:
	\begin{enumerate}
		\item Jeśli \( v \neq 0 \) to \( v \) jest wektorem własnym macierzy \( C = X^TX \) dla wartości własnej \( \lambda \)
		\item Jeśli \( \norm{a} = 1 \) to \( \norm{v} = \sqrt{\lambda} \)
	\end{enumerate}
\end{lemma}
\begin{proof} \( \)

	\begin{enumerate}
		\item
		      \begin{align*}
			      Ka                     & = \lambda a            \\
			      XX^Ta                  & = \lambda a            \\
			      X^T\pars{XX^Ta}        & = X^T \pars{\lambda a} \\
			      \pars{X^TX}\pars{X^Ta} & = \lambda X^T a        \\
			      Cv                     & = \lambda v
		      \end{align*}

		\item
		      \begin{align*}
			      \norm{v}^2
			       & = \norm{\sum_{i=1}^m a_i x^{(i)}}^2                               \\
			       & = \dotproduct{\sum_{i=1}^m a_i x^{(i)}, \sum_{i=1}^m a_i x^{(i)}} \\
			       & = \sum_{i=1}^m \sum_{j=1}^m a_i a_j\dotproduct{x^{(i)}, x^{(j)}}  \\
			       & = a^TKa                                                           \\
			       & = a^T\lambda a                                                    \\
			       & = \lambda
		      \end{align*}

	\end{enumerate}
\end{proof}

Mamy podobny lemat w drugą stronę

\begin{lemma}
	Niech \( Cv = \lambda v \), definiujemy \( a \)
	\[
		a = \frac{1}{\lambda}Xv
	\]
	wtedy \( a \) jest wektorem własnym macierzy Grama dla wartości \( \lambda \)
\end{lemma}
\begin{proof} \( \)

	Zaczniemy od pokazania, że niezerowe wektory własne macierzy \( C \) są liniowymi kombinacjami \( x^{(i)} \).

	Mamy
	\begin{align*}
		v
		 & = \frac{1}{\lambda} Cv                                          \\
		 & = \frac{1}{\lambda} \sum_{i=1}^m x^{(i)} x^{(i)^T}v             \\
		 & = \frac{1}{\lambda} \sum_{i=1}^m x^{(i)}\dotproduct{x^{(i)}, v} \\
		 & = \sum_{i=1}^m x^{(i)}\frac{1}{\lambda}\dotproduct{x^{(i)}, v}  \\
		 & = \sum_{i=1}^m x^{(i)} a_i
	\end{align*}

	Teraz pokażemy, jak wyrazić wektory własne macierzy \( C \) mając wektory własne macierzy \( K \)

	\begin{align*}
		Cv                                                                                        & = \lambda v                                  \\
		\pars{\sum_{i=1}^m x^{(i)}x^{(i)^T}}\pars{\sum_{i=1}^m x^{(i)} a_i}                       & = \lambda \sum_{i=1}^m a_i x^{(i)}           \\
		\sum_{i=1}^m \sum_{j=1}^m x^{(j)}x^{(j)^T} x^{(i)} a_i                                    & = \lambda \sum_{i=1}^m a_i x^{(i)}           \\
		x^{(n)^T} \pars{\sum_{i=1}^m \sum_{j=1}^m a_i x^{(j)} \dotproduct{x^{(j)}, x^{(i)}} }     & = \lambda \sum_{i=1}^m a_i x^{(n)^T} x^{(i)} \\
		\sum_{i=1}^m \sum_{j=1}^m a_i \dotproduct{x^{(n)}, x^{(j)}} \dotproduct{x^{(j)}, x^{(i)}} & = \lambda \sum_{i=1}^m a_i x^{(n)^T} x^{(i)} \\
		(K^2a)_n                                                                                  & = \lambda (Ka)_n                             \\
		Ka = \lambda a
	\end{align*}
\end{proof}

\subsection{Funkcje jądrowe}
Możemy teraz skorzystać z powyższych dwóch lematów i zaproponować poniższy algorytm:
\begin{enumerate}
	\item Wyznacz macierz Grama \( K \) i jej wektor własny \( a \)
	\item Znormalizuj \( a \) aby \( \norm{a} = 1 \)
	\item Oblicz \( v = \frac{1}{\sqrt{\lambda}} \sum_{i=1}^m a_i x^{(i)} \)
\end{enumerate}

Problem jest taki, że my korzystamy z \( x^{(i)} \) przy obliczaniu \( v \) -- chcemy się tego pozbyć.
Okazuje się, że nie musimy obliczać wektorów własnych per se, wystarczy nam znać projekcje zbioru danych na przestrzeń na nich rozpiętą

W przypadku jednowymiarowym mamy
\[
	v = \sum_{i=1}^m a_i x^{(i)}
\]
zatem
\[
	\pi_v(x^{(i)}) = v^Tx^{(i} = \sum_{j=1}^m a_j\dotproduct{x^{(j)}, x^{i}} =
	\sum_{j=1}^m a_i \kappa(x^{(j)}, x^{(i)})
\]

No i super -- możemy korzystać z dowolnych funkcji jądrowych.

Przydałoby się jeszcze przeskalować i uśrednić dane w przestrzeni cech, ale robimy to dość prosto.
Niech \( M \) będzie macierzą wypełnioną wartością \( \frac{1}{m} \) a \( K \) macierzą Grama po zastosowaniu funkcji jądrowych.

Obliczamy
\[
	K \leftarrow K - MK - KM + MKM
\]
po rozpisaniu tego mnożenia okaże się że istotnie dostaniemy \( K \) takie, jak gdyby wektory \( \phi(x^{(i)}) \) były uśrednione.

Algorytm jest bardzo podobny jak bez funkcji jądrowych tj.:
\begin{enumerate}
	\item Obliczamy macierz Grama \( K \)
	\item Środkujemy dane w przestrzeni cech
	\item Wyznaczamy rozkład SVD \( K = U\Sigma V^T \)
	\item Definiujemy \( V_r \)
	      \[
		      V_r = \begin{bmatrix}
			      \frac{1}{\sqrt{\lambda_1}} v_1 \hdots \frac{1}{\sqrt{\lambda_l}} v_l
		      \end{bmatrix}
	      \]
	\item Rzutujemy dane z macierzy Grama na przestrzeń rozpiętą na wektorach z \( V_r \)
	      \[
		      z^{(i)} = V_r^T K_i
	      \]
\end{enumerate}