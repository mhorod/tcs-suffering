\section{Definicje}

\subsection{Modele parametryczne}

\begin{definition}
	\textbf{Model parametryczny} z parametrami \( \theta \in \Theta \) to dowolny zbiór rozkładów prawdopodobieństwa \( \set{p(y \given \theta) : \theta \in \Theta} \)
\end{definition}

Przykładowe modele parametryczne to:
\begin{itemize}
	\item \(\Bern(p)\) -- pojedyncza próba z sukcesem \( p \)
	\item \(\Binom(n, p)\) -- rozkład dwumianowy, \( n \) prób z sukcesem \( p \)
	\item \(\Uni(a, b)\) -- rozkład jednostajny na przedziale \( (a, b) \)
	\item \(\Poi(\lambda)\) -- rozkład Poissona z parametrem \( \lambda \)
	\item \(\Normal(\mu, \sigma^2)\) -- rozkład normalny
	\item \(\Beta(\alpha, \beta \) -- rozkład beta
\end{itemize}

\newpage
\subsection{Rozkład Beta}
Nowością w stosunku do MPI jest tzw. rozkład beta \( \Beta(\alpha, \beta) \)
Jego funkcja gęstości wynosi
\[
	f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}
	\cdot x^{\alpha - 1}
	\cdot (1-x)^{\beta - 1}
\]
Na rysunku przedstawia on się następująco:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.25]{img/beta-distribution.png}
	\caption{Gęstość rozkładu beta dla różnych parametrów}
\end{figure}

Na ćwiczeniach pokazywaliśmy fakt, który warto znać, a mianowicie
\[
	\expected{\Beta(\alpha, \beta)} = \frac{\alpha}{\alpha + \beta}
\]

Ponadto mamy jeszcze jedną własność, którą można zauważyć na rysunku
\begin{itemize}
	\item dla \( \alpha, \beta < 1 \) rozkład jest bimodalny
	\item dla \( \alpha, \beta \geq 1 \) rozkład jest unimodalny
	\item dla \( \alpha = \beta \) rozkład jest symetryczny względem \( x = \frac{1}{2} \)
\end{itemize}

\subsection{Rozkłady sprzężone}
Aby nam się łatwiej liczyło wprowadzamy coś takiego jak \textbf{rozkład sprzężony}, czyli taki rozkład aprioryczny dla rozkładu próby, dla którego rozkład a posteriori i rozkład a priori jest taki sam.

\begin{center}
	\begin{tabular}{c c c}
		rozkład próby & a priori   & a posteriori \\
		\hline
		dwumianowy    & beta       & beta         \\
		Poissona      & gamma      & gamma        \\
		geometryczny  & beta       & beta         \\
		wielomianowy  & Dirichleta & Dirichleta   \\
		wykładniczy   & gamma      & gamma        \\
		normalny      & normalny   & normalny     \\
	\end{tabular}
\end{center}

Jest to głównie potrzebne po to aby nam się prościej liczyło.

\subsection{Estymator}

\begin{definition}
	\textbf{Estymator} to dowolna wartość szacowanego parametru obliczona na podstawie jakiejś próbki. Dla parametru \( \theta \) będziemy oznaczać estymator przez \( \widehat \theta \)
\end{definition}

\begin{definition}
	\textbf{Obciążenie estymatora} \( \widehat \theta \) parametru \( \theta \) definiujemy jako
	\[
		\bias(\widehat \theta) = \expected{\widehat \theta} - \theta
	\]

	Mówimy, że estymator jest \textbf{obciążony} jeśli \( \bias(\theta) \neq 0 \)
\end{definition}

\subsection{Twierdzenie Bayesa}

Ustalmy dowolny model parametryczny.
Niech \( \theta \) będzie zmienną opisującą parametr modelu.
Niech \( D \) będzie zmienną losową, która opisuje obserwacje zwracane przez ten model.
Zachodzi wtedy wzór Bayesa na prawdopodobieństwo warunkowe:
\[
	\prob(\theta = t \given D = d) = \frac{P(D = d \given \theta = t) \cdot P(\theta =t )}{P(D = d)}
\]

Dzięki temu jesteśmy w stanie na podstawie obserwacji szacować parametr naszego modelu.

Podobnie jeśli \( \theta \) i \( D \) są zmiennymi ciągłymi to
\[
	f_{\theta \given D = d} = \frac{f_{D \given \theta = t}(d) \cdot f_\theta(t)}{f_D(d)}
\]

Nadużywając notacji będziemy zapisywać obie te sytuacje jednocześnie, przyjmując przy tym że \( p(X) \) jest rozkładem zmiennej \( X \)

\[
	p(\theta \given D) = \frac{p(D \given \theta) \cdot p(\theta)}{p(D)}
\]

Nazywamy elementy tego wzoru:
\begin{itemize}
	\item \( p(\theta) \) to \textbf{prawdopodobieństwo a priori}, czyli co wiemy (zakładamy) o \( \theta \) jeśli nie mamy żadnych obserwacji
	\item \( p(D \mid \theta) \) to \textbf{wiarygodność}, czyli jak dobrze model z parametrem \( \theta \) przewiduje dane z obserwacji
	\item \( p(\theta \mid D) \) to \textbf{prawdopodobieństwo a posteriori}, czyli co powinniśmy zakładać o \( \theta \) jeśli poczyniliśmy już obserwacje
	\item \( p(D) \) to \textbf{wiarygodność brzegowa}, czyli jakie w ogólności jest prawdopodobieństwo uzyskania naszych obserwacji

	      \[
		      p(D) = \int_{\im \theta} p(D \mid \theta)p(\theta)d\theta
	      \]
\end{itemize}

Warto tutaj zauważyć, że \( p(D) \) jest niezależne od \( \theta \) tj. zależy jedynie od samego modelu, a nie od konkretnego parametru.
