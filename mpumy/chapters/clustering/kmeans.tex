\section{k-means}

\subsection{k-means}
Tutaj ustalamy a priori liczbę klastrów \( k \) i wykonujemy algorytm:
\begin{enumerate}
    \item Wylosuj \( k \) punktów \( \mu_1, \dots, \mu_k \) -- będą to ,,centroidy''  naszych klastrów
    \item Aż do stabilizacji powtarzaj:
    \begin{enumerate}
        \item Przydziel każdy punkt x do klastra najbliższego \( \mu_i \)
        \item Policz prawdziwe centroidy tak uzyskanych klastrów
    \end{enumerate}
\end{enumerate}

\subsection{k-means++}

Jeśli mamy pecha i wybierzemy słabe centroidy to k-means daje złe wyniki.
Aby sobie nieco z tym poradzić chcemy wybierać centroidy w trochę bardziej zbalansowany sposób -- tak aby były one mniej więcej równo rozłożone po przestrzeni.

Oczekiwany błąd jest co najwyżej \( 8(\lg k + 2) \) razy gorszy niż minimalny jaki możemy uzyskać.

\subsection{Wady i zalety}

Zalety:
\begin{itemize}
    \item Proste
    \item dobre dla dużych i kulistych danych
    \item działa w czasie liniowym
    \item klastry są zwarte
\end{itemize}

Wady:
\begin{itemize}
    \item wymaga wielu przebiegów ze względu na randomizację
    \item wymaga skalowania danych
    \item działa tylko na danych numerycznych
    \item nie radzi sobie z niekulistymi kształtami
\end{itemize}